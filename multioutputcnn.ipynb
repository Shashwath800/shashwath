{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955be674-d8ff-4954-bdcf-f00c9095cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 68ms/step - age_output_loss: 290.2667 - age_output_mae: 12.7258 - ethnicity_output_accuracy: 0.3216 - ethnicity_output_loss: 5.3128 - gender_output_accuracy: 0.5686 - gender_output_loss: 2.1440 - loss: 297.7237 - val_age_output_loss: 269.1617 - val_age_output_mae: 12.3734 - val_ethnicity_output_accuracy: 0.4927 - val_ethnicity_output_loss: 1.3271 - val_gender_output_accuracy: 0.5554 - val_gender_output_loss: 0.6778 - val_loss: 271.0390\n",
      "Epoch 2/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 66ms/step - age_output_loss: 149.3585 - age_output_mae: 9.0246 - ethnicity_output_accuracy: 0.4068 - ethnicity_output_loss: 1.5238 - gender_output_accuracy: 0.6253 - gender_output_loss: 0.6707 - loss: 151.5531 - val_age_output_loss: 338.7094 - val_age_output_mae: 15.4376 - val_ethnicity_output_accuracy: 0.4881 - val_ethnicity_output_loss: 1.3325 - val_gender_output_accuracy: 0.6100 - val_gender_output_loss: 0.6643 - val_loss: 341.0919\n",
      "Epoch 3/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 68ms/step - age_output_loss: 124.6616 - age_output_mae: 8.1890 - ethnicity_output_accuracy: 0.4195 - ethnicity_output_loss: 1.4701 - gender_output_accuracy: 0.6141 - gender_output_loss: 0.6585 - loss: 126.7903 - val_age_output_loss: 117.0355 - val_age_output_mae: 7.7432 - val_ethnicity_output_accuracy: 0.4727 - val_ethnicity_output_loss: 1.3690 - val_gender_output_accuracy: 0.6239 - val_gender_output_loss: 0.6277 - val_loss: 119.2171\n",
      "Epoch 4/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 110.0914 - age_output_mae: 7.7670 - ethnicity_output_accuracy: 0.4223 - ethnicity_output_loss: 1.4624 - gender_output_accuracy: 0.6200 - gender_output_loss: 0.6632 - loss: 112.2170 - val_age_output_loss: 176.2828 - val_age_output_mae: 9.3320 - val_ethnicity_output_accuracy: 0.4290 - val_ethnicity_output_loss: 1.3940 - val_gender_output_accuracy: 0.7190 - val_gender_output_loss: 0.5955 - val_loss: 178.6469\n",
      "Epoch 5/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 105.1701 - age_output_mae: 7.5936 - ethnicity_output_accuracy: 0.4144 - ethnicity_output_loss: 1.4785 - gender_output_accuracy: 0.6189 - gender_output_loss: 0.6581 - loss: 107.3068 - val_age_output_loss: 104.4070 - val_age_output_mae: 7.4716 - val_ethnicity_output_accuracy: 0.4410 - val_ethnicity_output_loss: 1.3377 - val_gender_output_accuracy: 0.6353 - val_gender_output_loss: 0.6049 - val_loss: 106.5927\n",
      "Epoch 6/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 72ms/step - age_output_loss: 97.1971 - age_output_mae: 7.2741 - ethnicity_output_accuracy: 0.4133 - ethnicity_output_loss: 1.4693 - gender_output_accuracy: 0.6230 - gender_output_loss: 0.6648 - loss: 99.3308 - val_age_output_loss: 141.4658 - val_age_output_mae: 8.2888 - val_ethnicity_output_accuracy: 0.4238 - val_ethnicity_output_loss: 1.4065 - val_gender_output_accuracy: 0.6984 - val_gender_output_loss: 0.6202 - val_loss: 143.4389\n",
      "Epoch 7/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 70ms/step - age_output_loss: 95.5268 - age_output_mae: 7.2393 - ethnicity_output_accuracy: 0.3948 - ethnicity_output_loss: 1.5194 - gender_output_accuracy: 0.6182 - gender_output_loss: 0.6831 - loss: 97.7292 - val_age_output_loss: 331.3097 - val_age_output_mae: 13.1590 - val_ethnicity_output_accuracy: 0.4514 - val_ethnicity_output_loss: 1.4552 - val_gender_output_accuracy: 0.6218 - val_gender_output_loss: 0.6375 - val_loss: 333.2954\n",
      "Epoch 8/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 88.7661 - age_output_mae: 7.0055 - ethnicity_output_accuracy: 0.4145 - ethnicity_output_loss: 1.4716 - gender_output_accuracy: 0.6435 - gender_output_loss: 0.6480 - loss: 90.8854 - val_age_output_loss: 131.6667 - val_age_output_mae: 8.2915 - val_ethnicity_output_accuracy: 0.5104 - val_ethnicity_output_loss: 1.3125 - val_gender_output_accuracy: 0.6927 - val_gender_output_loss: 0.6073 - val_loss: 133.9932\n",
      "Epoch 9/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 67ms/step - age_output_loss: 79.3116 - age_output_mae: 6.5840 - ethnicity_output_accuracy: 0.4060 - ethnicity_output_loss: 1.4769 - gender_output_accuracy: 0.6125 - gender_output_loss: 0.6817 - loss: 81.4703 - val_age_output_loss: 304.9657 - val_age_output_mae: 12.3244 - val_ethnicity_output_accuracy: 0.4503 - val_ethnicity_output_loss: 1.3629 - val_gender_output_accuracy: 0.5744 - val_gender_output_loss: 0.6487 - val_loss: 306.9844\n",
      "Epoch 10/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 87ms/step - age_output_loss: 80.2025 - age_output_mae: 6.6910 - ethnicity_output_accuracy: 0.3996 - ethnicity_output_loss: 1.5110 - gender_output_accuracy: 0.6154 - gender_output_loss: 0.6684 - loss: 82.3819 - val_age_output_loss: 91.0015 - val_age_output_mae: 7.0552 - val_ethnicity_output_accuracy: 0.4923 - val_ethnicity_output_loss: 1.3423 - val_gender_output_accuracy: 0.7013 - val_gender_output_loss: 0.5964 - val_loss: 93.4387\n",
      "Epoch 11/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 79ms/step - age_output_loss: 71.7414 - age_output_mae: 6.2888 - ethnicity_output_accuracy: 0.3933 - ethnicity_output_loss: 1.5204 - gender_output_accuracy: 0.6175 - gender_output_loss: 0.6665 - loss: 73.9284 - val_age_output_loss: 104.6162 - val_age_output_mae: 7.6941 - val_ethnicity_output_accuracy: 0.4328 - val_ethnicity_output_loss: 1.3426 - val_gender_output_accuracy: 0.5663 - val_gender_output_loss: 0.6633 - val_loss: 106.8993\n",
      "Epoch 12/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 94ms/step - age_output_loss: 68.1963 - age_output_mae: 6.1818 - ethnicity_output_accuracy: 0.3995 - ethnicity_output_loss: 1.5002 - gender_output_accuracy: 0.6235 - gender_output_loss: 0.6641 - loss: 70.3605 - val_age_output_loss: 91.5986 - val_age_output_mae: 6.9648 - val_ethnicity_output_accuracy: 0.4834 - val_ethnicity_output_loss: 1.3073 - val_gender_output_accuracy: 0.7250 - val_gender_output_loss: 0.5880 - val_loss: 93.9316\n",
      "Epoch 13/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 79ms/step - age_output_loss: 68.2252 - age_output_mae: 6.1672 - ethnicity_output_accuracy: 0.4048 - ethnicity_output_loss: 1.4784 - gender_output_accuracy: 0.6230 - gender_output_loss: 0.6639 - loss: 70.3673 - val_age_output_loss: 92.7626 - val_age_output_mae: 7.1815 - val_ethnicity_output_accuracy: 0.5003 - val_ethnicity_output_loss: 1.3538 - val_gender_output_accuracy: 0.7315 - val_gender_output_loss: 0.5663 - val_loss: 95.0358\n",
      "Epoch 14/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 76ms/step - age_output_loss: 65.4475 - age_output_mae: 6.0038 - ethnicity_output_accuracy: 0.3892 - ethnicity_output_loss: 1.5061 - gender_output_accuracy: 0.6342 - gender_output_loss: 0.6593 - loss: 67.6129 - val_age_output_loss: 171.0059 - val_age_output_mae: 10.3668 - val_ethnicity_output_accuracy: 0.4503 - val_ethnicity_output_loss: 1.3379 - val_gender_output_accuracy: 0.5630 - val_gender_output_loss: 0.6736 - val_loss: 172.8555\n",
      "Epoch 15/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 72ms/step - age_output_loss: 63.8200 - age_output_mae: 5.9063 - ethnicity_output_accuracy: 0.3987 - ethnicity_output_loss: 1.5026 - gender_output_accuracy: 0.6263 - gender_output_loss: 0.6737 - loss: 65.9963 - val_age_output_loss: 111.4183 - val_age_output_mae: 8.0280 - val_ethnicity_output_accuracy: 0.4617 - val_ethnicity_output_loss: 1.3660 - val_gender_output_accuracy: 0.7134 - val_gender_output_loss: 0.5871 - val_loss: 113.8320\n",
      "Epoch 16/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 94ms/step - age_output_loss: 59.9738 - age_output_mae: 5.7763 - ethnicity_output_accuracy: 0.4020 - ethnicity_output_loss: 1.4976 - gender_output_accuracy: 0.6291 - gender_output_loss: 0.6609 - loss: 62.1322 - val_age_output_loss: 190.8616 - val_age_output_mae: 10.2346 - val_ethnicity_output_accuracy: 0.4432 - val_ethnicity_output_loss: 1.3678 - val_gender_output_accuracy: 0.6572 - val_gender_output_loss: 0.6123 - val_loss: 192.9217\n",
      "Epoch 17/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 93ms/step - age_output_loss: 58.4283 - age_output_mae: 5.7118 - ethnicity_output_accuracy: 0.4014 - ethnicity_output_loss: 1.5164 - gender_output_accuracy: 0.6317 - gender_output_loss: 0.6620 - loss: 60.6067 - val_age_output_loss: 126.5223 - val_age_output_mae: 8.0167 - val_ethnicity_output_accuracy: 0.4396 - val_ethnicity_output_loss: 1.3373 - val_gender_output_accuracy: 0.7013 - val_gender_output_loss: 0.5891 - val_loss: 128.6269\n",
      "Epoch 18/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 78ms/step - age_output_loss: 55.1904 - age_output_mae: 5.5796 - ethnicity_output_accuracy: 0.4041 - ethnicity_output_loss: 1.4849 - gender_output_accuracy: 0.6444 - gender_output_loss: 0.6499 - loss: 57.3251 - val_age_output_loss: 135.9736 - val_age_output_mae: 8.3795 - val_ethnicity_output_accuracy: 0.4906 - val_ethnicity_output_loss: 1.3385 - val_gender_output_accuracy: 0.7250 - val_gender_output_loss: 0.5769 - val_loss: 138.2910\n",
      "Epoch 19/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 91ms/step - age_output_loss: 56.9382 - age_output_mae: 5.6001 - ethnicity_output_accuracy: 0.4106 - ethnicity_output_loss: 1.5033 - gender_output_accuracy: 0.6423 - gender_output_loss: 0.6543 - loss: 59.0958 - val_age_output_loss: 189.5952 - val_age_output_mae: 11.1507 - val_ethnicity_output_accuracy: 0.4569 - val_ethnicity_output_loss: 1.4966 - val_gender_output_accuracy: 0.6739 - val_gender_output_loss: 0.6083 - val_loss: 192.0951\n",
      "Epoch 20/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 105ms/step - age_output_loss: 54.2158 - age_output_mae: 5.5187 - ethnicity_output_accuracy: 0.4250 - ethnicity_output_loss: 1.4669 - gender_output_accuracy: 0.6284 - gender_output_loss: 0.6628 - loss: 56.3455 - val_age_output_loss: 90.6068 - val_age_output_mae: 6.8972 - val_ethnicity_output_accuracy: 0.4919 - val_ethnicity_output_loss: 1.2969 - val_gender_output_accuracy: 0.5897 - val_gender_output_loss: 0.6416 - val_loss: 92.8746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"age_gender.csv\")\n",
    "\n",
    "\n",
    "def process_pixels(pixel_str):\n",
    "    pixels = np.array(pixel_str.split(), dtype=\"uint8\")\n",
    "    return pixels.reshape(48, 48, 1)\n",
    "\n",
    "df[\"pixels\"] = df[\"pixels\"].apply(process_pixels)\n",
    "\n",
    "\n",
    "X = np.stack(df[\"pixels\"].values) / 255.0  \n",
    "y_age = df[\"age\"].values  \n",
    "y_gender = df[\"gender\"].values \n",
    "y_ethnicity = to_categorical(df[\"ethnicity\"].values) \n",
    "\n",
    "\n",
    "X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test, y_ethnicity_train, y_ethnicity_test = train_test_split(\n",
    "    X, y_age, y_gender, y_ethnicity, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(48, 48, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "age_output = Dense(1, activation=\"linear\", name=\"age_output\")(x)  \n",
    "gender_output = Dense(1, activation=\"sigmoid\", name=\"gender_output\")(x)  \n",
    "ethnicity_output = Dense(y_ethnicity.shape[1], activation=\"softmax\", name=\"ethnicity_output\")(x)  \n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[age_output, gender_output, ethnicity_output])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",  \n",
    "        \"gender_output\": \"binary_crossentropy\",  \n",
    "        \"ethnicity_output\": \"categorical_crossentropy\", \n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",  \n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    {\"age_output\": y_age_train, \"gender_output\": y_gender_train, \"ethnicity_output\": y_ethnicity_train},\n",
    "    validation_data=(X_test, {\"age_output\": y_age_test, \"gender_output\": y_gender_test, \"ethnicity_output\": y_ethnicity_test}),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "model.save(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93600ba5-4b2a-4933-92b7-6f35e85e447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60eb865b-2004-49b3-9200-fa5640c9d7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10263300-723e-4d73-b5c6-1ca294afcd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ethnicity_output    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m589,952\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ethnicity_output    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,368,400</span> (5.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,368,400\u001b[0m (5.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,975</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,975\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,977</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m683,977\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load model (change filename if using .keras)\n",
    "MODEL_PATH = \"multi_output_cnn.keras\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d185e4d-d699-46e7-97f7-8d8f0f485a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step\n",
      "Predicted Age: 48\n",
      "Predicted Gender: Male\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a dummy grayscale image (48x48)\n",
    "dummy_image = np.random.rand(1, 48, 48, 1)  # Batch size 1\n",
    "\n",
    "# Make prediction\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(dummy_image)\n",
    "\n",
    "# Process outputs\n",
    "age = round(age_pred[0][0])  # Convert to integer\n",
    "gender = \"Female\" if gender_pred[0][0] > 0.5 else \"Male\"\n",
    "ethnicity = np.argmax(ethnicity_pred[0])  # Get class index\n",
    "\n",
    "print(f\"Predicted Age: {age}\")\n",
    "print(f\"Predicted Gender: {gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d973c-9ca8-4d0f-b656-bd2057a0a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step\n",
      "Predicted Age: 51\n",
      "Predicted Gender: Male\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode=\"grayscale\", target_size=(48, 48))\n",
    "    img = img_to_array(img) / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Path to your test image\n",
    "image_path = \"your_image.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Make prediction\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(input_image)\n",
    "\n",
    "# Process results\n",
    "age = round(age_pred[0][0])\n",
    "gender = \"Female\" if gender_pred[0][0] > 0.5 else \"Male\"\n",
    "ethnicity = np.argmax(ethnicity_pred[0])\n",
    "\n",
    "print(f\"Predicted Age: {age}\")\n",
    "print(f\"Predicted Gender: {gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d79b9875-19dc-4812-ae12-a1db9667e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "Predicted Age: 50\n",
      "Predicted Gender: Female\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Function to preprocess input image\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=[0, -1])  # Expand dimensions for model input\n",
    "    return img\n",
    "\n",
    "# Predict on a test image\n",
    "img_path = \"mohith.jpg\"  # Replace with actual image path\n",
    "img = preprocess_image(img_path)\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(img)\n",
    "\n",
    "# Convert predictions to human-readable format\n",
    "predicted_age = int(age_pred[0][0])\n",
    "predicted_gender = \"Male\" if gender_pred[0][0] > 0.5 else \"Female\"\n",
    "predicted_ethnicity = np.argmax(ethnicity_pred[0])  # Get highest probability class\n",
    "\n",
    "print(f\"Predicted Age: {predicted_age}\")\n",
    "print(f\"Predicted Gender: {predicted_gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {predicted_ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2811aa1-cea9-4e90-a2e5-b82432c1a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DC0C65B6A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "Predicted Age: 50\n",
      "Predicted Gender: Female\n",
      "Predicted Ethnicity: Black\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = tf.keras.models.load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Define ethnicity class labels\n",
    "ethnicity_classes = {0: \"White\", 1: \"Black\", 2: \"Asian\", 3: \"Indian\", 4: \"Others\"}\n",
    "\n",
    "# Function to preprocess input image\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=[0, -1])  # Expand dimensions for model input\n",
    "    return img\n",
    "\n",
    "# Predict on a new image\n",
    "def predict_image(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    age_pred, gender_pred, ethnicity_pred = model.predict(img)\n",
    "\n",
    "    # Convert predictions\n",
    "    predicted_age = int(age_pred[0][0])  # Age output\n",
    "    predicted_gender = \"Male\" if gender_pred[0][0] > 0.5 else \"Female\"  # Gender classification\n",
    "    predicted_ethnicity = ethnicity_classes[np.argmax(ethnicity_pred[0])]  # Ethnicity classification\n",
    "\n",
    "    print(f\"Predicted Age: {predicted_age}\")\n",
    "    print(f\"Predicted Gender: {predicted_gender}\")\n",
    "    print(f\"Predicted Ethnicity: {predicted_ethnicity}\")\n",
    "\n",
    "# Test the script with an image\n",
    "image_path = \"mohith.jpg\"  # Replace with actual image path\n",
    "predict_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3ae6b6-1ecb-41d3-b263-b05d9488c4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Unfreeze some layers for fine-tuning\n",
    "for layer in model.layers[-10:]:  # Unfreezing last 10 layers\n",
    "    if hasattr(layer, \"trainable\"):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",\n",
    "        \"gender_output\": \"binary_crossentropy\",\n",
    "        \"ethnicity_output\": \"categorical_crossentropy\",\n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",\n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

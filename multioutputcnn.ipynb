{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "955be674-d8ff-4954-bdcf-f00c9095cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 68ms/step - age_output_loss: 290.2667 - age_output_mae: 12.7258 - ethnicity_output_accuracy: 0.3216 - ethnicity_output_loss: 5.3128 - gender_output_accuracy: 0.5686 - gender_output_loss: 2.1440 - loss: 297.7237 - val_age_output_loss: 269.1617 - val_age_output_mae: 12.3734 - val_ethnicity_output_accuracy: 0.4927 - val_ethnicity_output_loss: 1.3271 - val_gender_output_accuracy: 0.5554 - val_gender_output_loss: 0.6778 - val_loss: 271.0390\n",
      "Epoch 2/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 66ms/step - age_output_loss: 149.3585 - age_output_mae: 9.0246 - ethnicity_output_accuracy: 0.4068 - ethnicity_output_loss: 1.5238 - gender_output_accuracy: 0.6253 - gender_output_loss: 0.6707 - loss: 151.5531 - val_age_output_loss: 338.7094 - val_age_output_mae: 15.4376 - val_ethnicity_output_accuracy: 0.4881 - val_ethnicity_output_loss: 1.3325 - val_gender_output_accuracy: 0.6100 - val_gender_output_loss: 0.6643 - val_loss: 341.0919\n",
      "Epoch 3/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 68ms/step - age_output_loss: 124.6616 - age_output_mae: 8.1890 - ethnicity_output_accuracy: 0.4195 - ethnicity_output_loss: 1.4701 - gender_output_accuracy: 0.6141 - gender_output_loss: 0.6585 - loss: 126.7903 - val_age_output_loss: 117.0355 - val_age_output_mae: 7.7432 - val_ethnicity_output_accuracy: 0.4727 - val_ethnicity_output_loss: 1.3690 - val_gender_output_accuracy: 0.6239 - val_gender_output_loss: 0.6277 - val_loss: 119.2171\n",
      "Epoch 4/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 110.0914 - age_output_mae: 7.7670 - ethnicity_output_accuracy: 0.4223 - ethnicity_output_loss: 1.4624 - gender_output_accuracy: 0.6200 - gender_output_loss: 0.6632 - loss: 112.2170 - val_age_output_loss: 176.2828 - val_age_output_mae: 9.3320 - val_ethnicity_output_accuracy: 0.4290 - val_ethnicity_output_loss: 1.3940 - val_gender_output_accuracy: 0.7190 - val_gender_output_loss: 0.5955 - val_loss: 178.6469\n",
      "Epoch 5/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 105.1701 - age_output_mae: 7.5936 - ethnicity_output_accuracy: 0.4144 - ethnicity_output_loss: 1.4785 - gender_output_accuracy: 0.6189 - gender_output_loss: 0.6581 - loss: 107.3068 - val_age_output_loss: 104.4070 - val_age_output_mae: 7.4716 - val_ethnicity_output_accuracy: 0.4410 - val_ethnicity_output_loss: 1.3377 - val_gender_output_accuracy: 0.6353 - val_gender_output_loss: 0.6049 - val_loss: 106.5927\n",
      "Epoch 6/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 72ms/step - age_output_loss: 97.1971 - age_output_mae: 7.2741 - ethnicity_output_accuracy: 0.4133 - ethnicity_output_loss: 1.4693 - gender_output_accuracy: 0.6230 - gender_output_loss: 0.6648 - loss: 99.3308 - val_age_output_loss: 141.4658 - val_age_output_mae: 8.2888 - val_ethnicity_output_accuracy: 0.4238 - val_ethnicity_output_loss: 1.4065 - val_gender_output_accuracy: 0.6984 - val_gender_output_loss: 0.6202 - val_loss: 143.4389\n",
      "Epoch 7/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 70ms/step - age_output_loss: 95.5268 - age_output_mae: 7.2393 - ethnicity_output_accuracy: 0.3948 - ethnicity_output_loss: 1.5194 - gender_output_accuracy: 0.6182 - gender_output_loss: 0.6831 - loss: 97.7292 - val_age_output_loss: 331.3097 - val_age_output_mae: 13.1590 - val_ethnicity_output_accuracy: 0.4514 - val_ethnicity_output_loss: 1.4552 - val_gender_output_accuracy: 0.6218 - val_gender_output_loss: 0.6375 - val_loss: 333.2954\n",
      "Epoch 8/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 69ms/step - age_output_loss: 88.7661 - age_output_mae: 7.0055 - ethnicity_output_accuracy: 0.4145 - ethnicity_output_loss: 1.4716 - gender_output_accuracy: 0.6435 - gender_output_loss: 0.6480 - loss: 90.8854 - val_age_output_loss: 131.6667 - val_age_output_mae: 8.2915 - val_ethnicity_output_accuracy: 0.5104 - val_ethnicity_output_loss: 1.3125 - val_gender_output_accuracy: 0.6927 - val_gender_output_loss: 0.6073 - val_loss: 133.9932\n",
      "Epoch 9/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 67ms/step - age_output_loss: 79.3116 - age_output_mae: 6.5840 - ethnicity_output_accuracy: 0.4060 - ethnicity_output_loss: 1.4769 - gender_output_accuracy: 0.6125 - gender_output_loss: 0.6817 - loss: 81.4703 - val_age_output_loss: 304.9657 - val_age_output_mae: 12.3244 - val_ethnicity_output_accuracy: 0.4503 - val_ethnicity_output_loss: 1.3629 - val_gender_output_accuracy: 0.5744 - val_gender_output_loss: 0.6487 - val_loss: 306.9844\n",
      "Epoch 10/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 87ms/step - age_output_loss: 80.2025 - age_output_mae: 6.6910 - ethnicity_output_accuracy: 0.3996 - ethnicity_output_loss: 1.5110 - gender_output_accuracy: 0.6154 - gender_output_loss: 0.6684 - loss: 82.3819 - val_age_output_loss: 91.0015 - val_age_output_mae: 7.0552 - val_ethnicity_output_accuracy: 0.4923 - val_ethnicity_output_loss: 1.3423 - val_gender_output_accuracy: 0.7013 - val_gender_output_loss: 0.5964 - val_loss: 93.4387\n",
      "Epoch 11/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 79ms/step - age_output_loss: 71.7414 - age_output_mae: 6.2888 - ethnicity_output_accuracy: 0.3933 - ethnicity_output_loss: 1.5204 - gender_output_accuracy: 0.6175 - gender_output_loss: 0.6665 - loss: 73.9284 - val_age_output_loss: 104.6162 - val_age_output_mae: 7.6941 - val_ethnicity_output_accuracy: 0.4328 - val_ethnicity_output_loss: 1.3426 - val_gender_output_accuracy: 0.5663 - val_gender_output_loss: 0.6633 - val_loss: 106.8993\n",
      "Epoch 12/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 94ms/step - age_output_loss: 68.1963 - age_output_mae: 6.1818 - ethnicity_output_accuracy: 0.3995 - ethnicity_output_loss: 1.5002 - gender_output_accuracy: 0.6235 - gender_output_loss: 0.6641 - loss: 70.3605 - val_age_output_loss: 91.5986 - val_age_output_mae: 6.9648 - val_ethnicity_output_accuracy: 0.4834 - val_ethnicity_output_loss: 1.3073 - val_gender_output_accuracy: 0.7250 - val_gender_output_loss: 0.5880 - val_loss: 93.9316\n",
      "Epoch 13/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 79ms/step - age_output_loss: 68.2252 - age_output_mae: 6.1672 - ethnicity_output_accuracy: 0.4048 - ethnicity_output_loss: 1.4784 - gender_output_accuracy: 0.6230 - gender_output_loss: 0.6639 - loss: 70.3673 - val_age_output_loss: 92.7626 - val_age_output_mae: 7.1815 - val_ethnicity_output_accuracy: 0.5003 - val_ethnicity_output_loss: 1.3538 - val_gender_output_accuracy: 0.7315 - val_gender_output_loss: 0.5663 - val_loss: 95.0358\n",
      "Epoch 14/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 76ms/step - age_output_loss: 65.4475 - age_output_mae: 6.0038 - ethnicity_output_accuracy: 0.3892 - ethnicity_output_loss: 1.5061 - gender_output_accuracy: 0.6342 - gender_output_loss: 0.6593 - loss: 67.6129 - val_age_output_loss: 171.0059 - val_age_output_mae: 10.3668 - val_ethnicity_output_accuracy: 0.4503 - val_ethnicity_output_loss: 1.3379 - val_gender_output_accuracy: 0.5630 - val_gender_output_loss: 0.6736 - val_loss: 172.8555\n",
      "Epoch 15/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 72ms/step - age_output_loss: 63.8200 - age_output_mae: 5.9063 - ethnicity_output_accuracy: 0.3987 - ethnicity_output_loss: 1.5026 - gender_output_accuracy: 0.6263 - gender_output_loss: 0.6737 - loss: 65.9963 - val_age_output_loss: 111.4183 - val_age_output_mae: 8.0280 - val_ethnicity_output_accuracy: 0.4617 - val_ethnicity_output_loss: 1.3660 - val_gender_output_accuracy: 0.7134 - val_gender_output_loss: 0.5871 - val_loss: 113.8320\n",
      "Epoch 16/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 94ms/step - age_output_loss: 59.9738 - age_output_mae: 5.7763 - ethnicity_output_accuracy: 0.4020 - ethnicity_output_loss: 1.4976 - gender_output_accuracy: 0.6291 - gender_output_loss: 0.6609 - loss: 62.1322 - val_age_output_loss: 190.8616 - val_age_output_mae: 10.2346 - val_ethnicity_output_accuracy: 0.4432 - val_ethnicity_output_loss: 1.3678 - val_gender_output_accuracy: 0.6572 - val_gender_output_loss: 0.6123 - val_loss: 192.9217\n",
      "Epoch 17/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 93ms/step - age_output_loss: 58.4283 - age_output_mae: 5.7118 - ethnicity_output_accuracy: 0.4014 - ethnicity_output_loss: 1.5164 - gender_output_accuracy: 0.6317 - gender_output_loss: 0.6620 - loss: 60.6067 - val_age_output_loss: 126.5223 - val_age_output_mae: 8.0167 - val_ethnicity_output_accuracy: 0.4396 - val_ethnicity_output_loss: 1.3373 - val_gender_output_accuracy: 0.7013 - val_gender_output_loss: 0.5891 - val_loss: 128.6269\n",
      "Epoch 18/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 78ms/step - age_output_loss: 55.1904 - age_output_mae: 5.5796 - ethnicity_output_accuracy: 0.4041 - ethnicity_output_loss: 1.4849 - gender_output_accuracy: 0.6444 - gender_output_loss: 0.6499 - loss: 57.3251 - val_age_output_loss: 135.9736 - val_age_output_mae: 8.3795 - val_ethnicity_output_accuracy: 0.4906 - val_ethnicity_output_loss: 1.3385 - val_gender_output_accuracy: 0.7250 - val_gender_output_loss: 0.5769 - val_loss: 138.2910\n",
      "Epoch 19/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 91ms/step - age_output_loss: 56.9382 - age_output_mae: 5.6001 - ethnicity_output_accuracy: 0.4106 - ethnicity_output_loss: 1.5033 - gender_output_accuracy: 0.6423 - gender_output_loss: 0.6543 - loss: 59.0958 - val_age_output_loss: 189.5952 - val_age_output_mae: 11.1507 - val_ethnicity_output_accuracy: 0.4569 - val_ethnicity_output_loss: 1.4966 - val_gender_output_accuracy: 0.6739 - val_gender_output_loss: 0.6083 - val_loss: 192.0951\n",
      "Epoch 20/20\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 105ms/step - age_output_loss: 54.2158 - age_output_mae: 5.5187 - ethnicity_output_accuracy: 0.4250 - ethnicity_output_loss: 1.4669 - gender_output_accuracy: 0.6284 - gender_output_loss: 0.6628 - loss: 56.3455 - val_age_output_loss: 90.6068 - val_age_output_mae: 6.8972 - val_ethnicity_output_accuracy: 0.4919 - val_ethnicity_output_loss: 1.2969 - val_gender_output_accuracy: 0.5897 - val_gender_output_loss: 0.6416 - val_loss: 92.8746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"age_gender.csv\")\n",
    "\n",
    "\n",
    "def process_pixels(pixel_str):\n",
    "    pixels = np.array(pixel_str.split(), dtype=\"uint8\")\n",
    "    return pixels.reshape(48, 48, 1)\n",
    "\n",
    "df[\"pixels\"] = df[\"pixels\"].apply(process_pixels)\n",
    "\n",
    "\n",
    "X = np.stack(df[\"pixels\"].values) / 255.0  \n",
    "y_age = df[\"age\"].values  \n",
    "y_gender = df[\"gender\"].values \n",
    "y_ethnicity = to_categorical(df[\"ethnicity\"].values) \n",
    "\n",
    "\n",
    "X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test, y_ethnicity_train, y_ethnicity_test = train_test_split(\n",
    "    X, y_age, y_gender, y_ethnicity, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(48, 48, 1))\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "age_output = Dense(1, activation=\"linear\", name=\"age_output\")(x)  \n",
    "gender_output = Dense(1, activation=\"sigmoid\", name=\"gender_output\")(x)  \n",
    "ethnicity_output = Dense(y_ethnicity.shape[1], activation=\"softmax\", name=\"ethnicity_output\")(x)  \n",
    "\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=[age_output, gender_output, ethnicity_output])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",  \n",
    "        \"gender_output\": \"binary_crossentropy\",  \n",
    "        \"ethnicity_output\": \"categorical_crossentropy\", \n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",  \n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    X_train,\n",
    "    {\"age_output\": y_age_train, \"gender_output\": y_gender_train, \"ethnicity_output\": y_ethnicity_train},\n",
    "    validation_data=(X_test, {\"age_output\": y_age_test, \"gender_output\": y_gender_test, \"ethnicity_output\": y_ethnicity_test}),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "\n",
    "model.save(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93600ba5-4b2a-4933-92b7-6f35e85e447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60eb865b-2004-49b3-9200-fa5640c9d7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10263300-723e-4d73-b5c6-1ca294afcd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">589,952</span> │ flatten[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ethnicity_output    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │ dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │    \u001b[38;5;34m589,952\u001b[0m │ flatten[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ age_output (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ gender_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ ethnicity_output    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │        \u001b[38;5;34m645\u001b[0m │ dropout[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,368,400</span> (5.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,368,400\u001b[0m (5.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,975</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m683,975\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">683,977</span> (2.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m683,977\u001b[0m (2.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load model (change filename if using .keras)\n",
    "MODEL_PATH = \"multi_output_cnn.keras\"\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d185e4d-d699-46e7-97f7-8d8f0f485a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 335ms/step\n",
      "Predicted Age: 48\n",
      "Predicted Gender: Male\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a dummy grayscale image (48x48)\n",
    "dummy_image = np.random.rand(1, 48, 48, 1)  # Batch size 1\n",
    "\n",
    "# Make prediction\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(dummy_image)\n",
    "\n",
    "# Process outputs\n",
    "age = round(age_pred[0][0])  # Convert to integer\n",
    "gender = \"Female\" if gender_pred[0][0] > 0.5 else \"Male\"\n",
    "ethnicity = np.argmax(ethnicity_pred[0])  # Get class index\n",
    "\n",
    "print(f\"Predicted Age: {age}\")\n",
    "print(f\"Predicted Gender: {gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d973c-9ca8-4d0f-b656-bd2057a0a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 368ms/step\n",
      "Predicted Age: 51\n",
      "Predicted Gender: Male\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode=\"grayscale\", target_size=(48, 48))\n",
    "    img = img_to_array(img) / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Path to your test image\n",
    "image_path = \"your_image.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Make prediction\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(input_image)\n",
    "\n",
    "# Process results\n",
    "age = round(age_pred[0][0])\n",
    "gender = \"Female\" if gender_pred[0][0] > 0.5 else \"Male\"\n",
    "ethnicity = np.argmax(ethnicity_pred[0])\n",
    "\n",
    "print(f\"Predicted Age: {age}\")\n",
    "print(f\"Predicted Gender: {gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d79b9875-19dc-4812-ae12-a1db9667e775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 256ms/step\n",
      "Predicted Age: 50\n",
      "Predicted Gender: Female\n",
      "Predicted Ethnicity Class: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Function to preprocess input image\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=[0, -1])  # Expand dimensions for model input\n",
    "    return img\n",
    "\n",
    "# Predict on a test image\n",
    "img_path = \"mohith.jpg\"  # Replace with actual image path\n",
    "img = preprocess_image(img_path)\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(img)\n",
    "\n",
    "# Convert predictions to human-readable format\n",
    "predicted_age = int(age_pred[0][0])\n",
    "predicted_gender = \"Male\" if gender_pred[0][0] > 0.5 else \"Female\"\n",
    "predicted_ethnicity = np.argmax(ethnicity_pred[0])  # Get highest probability class\n",
    "\n",
    "print(f\"Predicted Age: {predicted_age}\")\n",
    "print(f\"Predicted Gender: {predicted_gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {predicted_ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2811aa1-cea9-4e90-a2e5-b82432c1a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001DC0C65B6A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 236ms/step\n",
      "Predicted Age: 50\n",
      "Predicted Gender: Female\n",
      "Predicted Ethnicity: Black\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = tf.keras.models.load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Define ethnicity class labels\n",
    "ethnicity_classes = {0: \"White\", 1: \"Black\", 2: \"Asian\", 3: \"Indian\", 4: \"Others\"}\n",
    "\n",
    "# Function to preprocess input image\n",
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "    img = cv2.resize(img, (48, 48))  # Resize to match model input\n",
    "    img = img.astype(\"float32\") / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=[0, -1])  # Expand dimensions for model input\n",
    "    return img\n",
    "\n",
    "# Predict on a new image\n",
    "def predict_image(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    age_pred, gender_pred, ethnicity_pred = model.predict(img)\n",
    "\n",
    "    # Convert predictions\n",
    "    predicted_age = int(age_pred[0][0])  # Age output\n",
    "    predicted_gender = \"Male\" if gender_pred[0][0] > 0.5 else \"Female\"  # Gender classification\n",
    "    predicted_ethnicity = ethnicity_classes[np.argmax(ethnicity_pred[0])]  # Ethnicity classification\n",
    "\n",
    "    print(f\"Predicted Age: {predicted_age}\")\n",
    "    print(f\"Predicted Gender: {predicted_gender}\")\n",
    "    print(f\"Predicted Ethnicity: {predicted_ethnicity}\")\n",
    "\n",
    "# Test the script with an image\n",
    "image_path = \"mohith.jpg\"  # Replace with actual image path\n",
    "predict_image(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b3ae6b6-1ecb-41d3-b263-b05d9488c4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Unfreeze some layers for fine-tuning\n",
    "for layer in model.layers[-10:]:  # Unfreezing last 10 layers\n",
    "    if hasattr(layer, \"trainable\"):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Lower learning rate for fine-tuning\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",\n",
    "        \"gender_output\": \"binary_crossentropy\",\n",
    "        \"ethnicity_output\": \"categorical_crossentropy\",\n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",\n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9aee74e-93ab-4997-a2d7-7ddd26f891d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      3\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      4\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      5\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     11\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m datagen\u001b[38;5;241m.\u001b[39mfit(X_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbf5258-a364-4393-a3b8-4aec517a60a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m      6\u001b[0m datagen \u001b[38;5;241m=\u001b[39m ImageDataGenerator(\n\u001b[0;32m      7\u001b[0m     rotation_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      8\u001b[0m     width_shift_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     shear_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     14\u001b[0m )\n\u001b[1;32m---> 16\u001b[0m datagen\u001b[38;5;241m.\u001b[39mfit(X_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"age_gender.csv\")\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    shear_range=0.2\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688e3002-7e21-4eba-8e20-25195c73f231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (18964, 48, 48, 1), y.shape = ()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 79\u001b[0m\n\u001b[0;32m     75\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Continue training (fine-tuning)\u001b[39;00m\n\u001b[0;32m     78\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m---> 79\u001b[0m     datagen\u001b[38;5;241m.\u001b[39mflow(X_train, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_age_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_gender_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124methnicity_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_ethnicity_train}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     80\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_age_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_gender_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124methnicity_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_ethnicity_test}),\n\u001b[0;32m     81\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Increase epochs for fine-tuning\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m64\u001b[39m,  \u001b[38;5;66;03m# Ensures full dataset coverage\u001b[39;00m\n\u001b[0;32m     83\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: gender_class_weights},\n\u001b[0;32m     84\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[lr_scheduler, early_stopping]\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_output_cnn_finetuned.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1103\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1091\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1102\u001b[0m ):\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NumpyArrayIterator(\n\u001b[0;32m   1104\u001b[0m         x,\n\u001b[0;32m   1105\u001b[0m         y,\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1107\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1108\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1109\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1110\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1111\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1112\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1113\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1114\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1115\u001b[0m         ignore_class_split\u001b[38;5;241m=\u001b[39mignore_class_split,\n\u001b[0;32m   1116\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:562\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    559\u001b[0m     x_misc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[1;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_weight):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(sample_weight)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (18964, 48, 48, 1), y.shape = ()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load dataset again\n",
    "df = pd.read_csv(\"age_gender.csv\")\n",
    "\n",
    "# Process pixel data\n",
    "def process_pixels(pixel_str):\n",
    "    return np.array(pixel_str.split(), dtype=\"uint8\").reshape(48, 48, 1)\n",
    "\n",
    "df[\"pixels\"] = df[\"pixels\"].apply(process_pixels)\n",
    "\n",
    "# Normalize images\n",
    "X = np.stack(df[\"pixels\"].values) / 255.0  \n",
    "\n",
    "# Extract labels\n",
    "y_age = df[\"age\"].values / df[\"age\"].max()  # Normalize age\n",
    "y_gender = df[\"gender\"].values\n",
    "y_ethnicity = to_categorical(df[\"ethnicity\"].values)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test, y_ethnicity_train, y_ethnicity_test = train_test_split(\n",
    "    X, y_age, y_gender, y_ethnicity, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Compute class weights for gender (handle imbalanced data)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_gender), y=y_gender)\n",
    "gender_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Unfreeze last 10 layers for fine-tuning\n",
    "for layer in model.layers[-10:]:\n",
    "    if hasattr(layer, \"trainable\"):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Small learning rate for fine-tuning\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",\n",
    "        \"gender_output\": \"binary_crossentropy\",\n",
    "        \"ethnicity_output\": \"categorical_crossentropy\",\n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",\n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Data Augmentation for Fine-Tuning\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    shear_range=0.2\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Callbacks for fine-tuning\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "model.fit(\n",
    "    datagen.flow(X_train, {\"age_output\": y_age_train, \"gender_output\": y_gender_train, \"ethnicity_output\": y_ethnicity_train}, batch_size=64),\n",
    "    validation_data=(X_test, {\"age_output\": y_age_test, \"gender_output\": y_gender_test, \"ethnicity_output\": y_ethnicity_test}),\n",
    "    epochs=30,  # Increase epochs for fine-tuning\n",
    "    steps_per_epoch=len(X_train) // 64,  # Ensures full dataset coverage\n",
    "    class_weight={\"gender_output\": gender_class_weights},\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"multi_output_cnn_finetuned.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99b5cf2e-5d37-4e30-8e2c-5820faa74bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (23705, 48, 48, 1)\n",
      "y_age shape: (23705,)\n",
      "y_gender shape: (23705,)\n",
      "y_ethnicity shape: (23705, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (18964, 48, 48, 1), y.shape = ()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 89\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m X_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_age_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_gender_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_ethnicity_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in testing data sizes!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Continue training (fine-tuning)\u001b[39;00m\n\u001b[0;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m---> 89\u001b[0m     datagen\u001b[38;5;241m.\u001b[39mflow(X_train, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_age_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_gender_train, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124methnicity_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_ethnicity_train}, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m),\n\u001b[0;32m     90\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_age_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_gender_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124methnicity_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_ethnicity_test}),\n\u001b[0;32m     91\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m,  \u001b[38;5;66;03m# Increase epochs for fine-tuning\u001b[39;00m\n\u001b[0;32m     92\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(X_train) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m64\u001b[39m,  \u001b[38;5;66;03m# Ensures full dataset coverage\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     class_weight\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: gender_class_weights},\n\u001b[0;32m     94\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[lr_scheduler, early_stopping]\n\u001b[0;32m     95\u001b[0m )\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned model\u001b[39;00m\n\u001b[0;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_output_cnn_finetuned.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1103\u001b[0m, in \u001b[0;36mImageDataGenerator.flow\u001b[1;34m(self, x, y, batch_size, shuffle, sample_weight, seed, save_to_dir, save_prefix, save_format, ignore_class_split, subset)\u001b[0m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflow\u001b[39m(\n\u001b[0;32m   1090\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1091\u001b[0m     x,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     subset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1102\u001b[0m ):\n\u001b[1;32m-> 1103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NumpyArrayIterator(\n\u001b[0;32m   1104\u001b[0m         x,\n\u001b[0;32m   1105\u001b[0m         y,\n\u001b[0;32m   1106\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1107\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1108\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39mshuffle,\n\u001b[0;32m   1109\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m   1110\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[0;32m   1111\u001b[0m         data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format,\n\u001b[0;32m   1112\u001b[0m         save_to_dir\u001b[38;5;241m=\u001b[39msave_to_dir,\n\u001b[0;32m   1113\u001b[0m         save_prefix\u001b[38;5;241m=\u001b[39msave_prefix,\n\u001b[0;32m   1114\u001b[0m         save_format\u001b[38;5;241m=\u001b[39msave_format,\n\u001b[0;32m   1115\u001b[0m         ignore_class_split\u001b[38;5;241m=\u001b[39mignore_class_split,\n\u001b[0;32m   1116\u001b[0m         subset\u001b[38;5;241m=\u001b[39msubset,\n\u001b[0;32m   1117\u001b[0m         dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[0;32m   1118\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:562\u001b[0m, in \u001b[0;36mNumpyArrayIterator.__init__\u001b[1;34m(self, x, y, image_data_generator, batch_size, shuffle, sample_weight, seed, data_format, save_to_dir, save_prefix, save_format, subset, ignore_class_split, dtype)\u001b[0m\n\u001b[0;32m    559\u001b[0m     x_misc \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y):\n\u001b[1;32m--> 562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `y` (labels) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(y)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m     )\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(sample_weight):\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    570\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` (images tensor) and `sample_weight` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    571\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshould have the same length. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    572\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound: x.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    573\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight.shape = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39masarray(sample_weight)\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    574\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: `x` (images tensor) and `y` (labels) should have the same length. Found: x.shape = (18964, 48, 48, 1), y.shape = ()"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"age_gender.csv\")\n",
    "\n",
    "# Process pixel data\n",
    "def process_pixels(pixel_str):\n",
    "    return np.array(pixel_str.split(), dtype=\"uint8\").reshape(48, 48, 1)\n",
    "\n",
    "df[\"pixels\"] = df[\"pixels\"].apply(process_pixels)\n",
    "\n",
    "# Normalize images\n",
    "X = np.stack(df[\"pixels\"].values) / 255.0  \n",
    "\n",
    "# Ensure labels are correctly shaped\n",
    "y_age = df[\"age\"].values.astype(np.float32) / df[\"age\"].max()  # Normalize age\n",
    "y_gender = df[\"gender\"].values.astype(np.float32)  # Gender (0 or 1)\n",
    "y_ethnicity = to_categorical(df[\"ethnicity\"].values)  # One-hot encode ethnicity\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y_age shape: {y_age.shape}\")\n",
    "print(f\"y_gender shape: {y_gender.shape}\")\n",
    "print(f\"y_ethnicity shape: {y_ethnicity.shape}\")\n",
    "\n",
    "# Train-test split with stratification on gender\n",
    "X_train, X_test, y_age_train, y_age_test, y_gender_train, y_gender_test, y_ethnicity_train, y_ethnicity_test = train_test_split(\n",
    "    X, y_age, y_gender, y_ethnicity, test_size=0.2, random_state=42, stratify=y_gender\n",
    ")\n",
    "\n",
    "# Compute class weights for gender (handle imbalanced data)\n",
    "class_weights = compute_class_weight(\"balanced\", classes=np.unique(y_gender_train), y=y_gender_train)\n",
    "gender_class_weights = dict(enumerate(class_weights))\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")\n",
    "\n",
    "# Unfreeze last 10 layers for fine-tuning\n",
    "for layer in model.layers[-10:]:\n",
    "    if hasattr(layer, \"trainable\"):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Small learning rate for fine-tuning\n",
    "    loss={\n",
    "        \"age_output\": \"mse\",\n",
    "        \"gender_output\": \"binary_crossentropy\",\n",
    "        \"ethnicity_output\": \"categorical_crossentropy\",\n",
    "    },\n",
    "    metrics={\n",
    "        \"age_output\": \"mae\",\n",
    "        \"gender_output\": \"accuracy\",\n",
    "        \"ethnicity_output\": \"accuracy\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Data Augmentation for Fine-Tuning\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    shear_range=0.2\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Callbacks for fine-tuning\n",
    "lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True)\n",
    "\n",
    "# Final check before training\n",
    "assert X_train.shape[0] == y_age_train.shape[0] == y_gender_train.shape[0] == y_ethnicity_train.shape[0], \"Mismatch in training data sizes!\"\n",
    "assert X_test.shape[0] == y_age_test.shape[0] == y_gender_test.shape[0] == y_ethnicity_test.shape[0], \"Mismatch in testing data sizes!\"\n",
    "\n",
    "# Continue training (fine-tuning)\n",
    "model.fit(\n",
    "    datagen.flow(X_train, {\"age_output\": y_age_train, \"gender_output\": y_gender_train, \"ethnicity_output\": y_ethnicity_train}, batch_size=64),\n",
    "    validation_data=(X_test, {\"age_output\": y_age_test, \"gender_output\": y_gender_test, \"ethnicity_output\": y_ethnicity_test}),\n",
    "    epochs=30,  # Increase epochs for fine-tuning\n",
    "    steps_per_epoch=len(X_train) // 64,  # Ensures full dataset coverage\n",
    "    class_weight={\"gender_output\": gender_class_weights},\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save(\"multi_output_cnn_finetuned.keras\")\n",
    "\n",
    "print(\"Fine-tuning complete. Model saved as 'multi_output_cnn_finetuned.keras'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a49d4d9-4a11-497d-9d72-cdab2ecfc015",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'load'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_output_cnn.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Functional' object has no attribute 'load'"
     ]
    }
   ],
   "source": [
    "model.load(\"multi_output_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f4e8bc1-630e-4d6c-a156-a6b4477f054b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'classes' parameter of compute_class_weight must be an instance of 'numpy.ndarray'. Got [0, 1] instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 204\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m     trained_model, max_age \u001b[38;5;241m=\u001b[39m train_model()\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;66;03m# Example usage after training\u001b[39;00m\n\u001b[0;32m    207\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m load_and_predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_task_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_age)\n",
      "Cell \u001b[1;32mIn[13], line 114\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    111\u001b[0m ethnicity_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(ethnicity_classes), y\u001b[38;5;241m=\u001b[39methnicity_classes)\n\u001b[0;32m    112\u001b[0m ethnicity_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(ethnicity_weights))\n\u001b[1;32m--> 114\u001b[0m gender_weights \u001b[38;5;241m=\u001b[39m compute_class_weight(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, classes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], y\u001b[38;5;241m=\u001b[39my_gender_train)\n\u001b[0;32m    115\u001b[0m gender_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28menumerate\u001b[39m(gender_weights))\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# Compile model\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m validate_parameter_constraints(\n\u001b[0;32m    204\u001b[0m     parameter_constraints, params, caller_name\u001b[38;5;241m=\u001b[39mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'classes' parameter of compute_class_weight must be an instance of 'numpy.ndarray'. Got [0, 1] instead."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D,\n",
    "                                     Dense, Dropout, BatchNormalization, RandomFlip,\n",
    "                                     RandomRotation, RandomZoom, RandomContrast)\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Load and preprocess data\n",
    "def load_data():\n",
    "    df = pd.read_csv(\"age_gender.csv\")\n",
    "    \n",
    "    # Process pixels\n",
    "    df[\"pixels\"] = df[\"pixels\"].apply(\n",
    "        lambda x: np.array(x.split(), dtype=\"uint8\").reshape(48, 48, 1)\n",
    "    )\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    X = np.stack(df[\"pixels\"].values) / 255.0\n",
    "    \n",
    "    # Process labels\n",
    "    max_age = df[\"age\"].max()\n",
    "    y_age = df[\"age\"].values / max_age  # Normalize age 0-1\n",
    "    y_gender = df[\"gender\"].values\n",
    "    y_ethnicity = to_categorical(df[\"ethnicity\"].values)\n",
    "    \n",
    "    return X, y_age, y_gender, y_ethnicity, max_age\n",
    "\n",
    "# Build enhanced model\n",
    "def build_multi_task_model(input_shape=(48, 48, 1), num_ethnicities=5):\n",
    "    # Data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        RandomFlip(\"horizontal\"),\n",
    "        RandomRotation(0.1),\n",
    "        RandomZoom(0.2),\n",
    "        RandomContrast(0.1),\n",
    "    ])\n",
    "    \n",
    "    # Input and augmentation\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = data_augmentation(input_layer)\n",
    "    \n",
    "    # Feature extractor\n",
    "    x = Conv2D(64, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(128, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = MaxPooling2D(2, 2)(x)\n",
    "    \n",
    "    x = Conv2D(256, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    # Shared dense layers\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = tf.keras.layers.LeakyReLU(0.2)(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    # Task-specific branches\n",
    "    # Age branch\n",
    "    age_branch = Dense(256)(x)\n",
    "    age_branch = BatchNormalization()(age_branch)\n",
    "    age_branch = tf.keras.layers.LeakyReLU(0.2)(age_branch)\n",
    "    age_branch = Dropout(0.3)(age_branch)\n",
    "    age_output = Dense(1, activation='linear', name='age_output')(age_branch)\n",
    "    \n",
    "    # Gender branch\n",
    "    gender_branch = Dense(128)(x)\n",
    "    gender_branch = BatchNormalization()(gender_branch)\n",
    "    gender_branch = tf.keras.layers.LeakyReLU(0.2)(gender_branch)\n",
    "    gender_branch = Dropout(0.3)(gender_branch)\n",
    "    gender_output = Dense(1, activation='sigmoid', name='gender_output')(gender_branch)\n",
    "    \n",
    "    # Ethnicity branch\n",
    "    ethnicity_branch = Dense(256)(x)\n",
    "    ethnicity_branch = BatchNormalization()(ethnicity_branch)\n",
    "    ethnicity_branch = tf.keras.layers.LeakyReLU(0.2)(ethnicity_branch)\n",
    "    ethnicity_branch = Dropout(0.4)(ethnicity_branch)\n",
    "    ethnicity_output = Dense(num_ethnicities, activation='softmax', name='ethnicity_output')(ethnicity_branch)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=[age_output, gender_output, ethnicity_output])\n",
    "\n",
    "# Main training function\n",
    "def train_model():\n",
    "    # Load and prepare data\n",
    "    X, y_age, y_gender, y_ethnicity, max_age = load_data()\n",
    "    \n",
    "    # Split data\n",
    "    (X_train, X_test,\n",
    "     y_age_train, y_age_test,\n",
    "     y_gender_train, y_gender_test,\n",
    "     y_ethnicity_train, y_ethnicity_test) = train_test_split(\n",
    "        X, y_age, y_gender, y_ethnicity,\n",
    "        test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Build model\n",
    "    model = build_multi_task_model()\n",
    "    \n",
    "    # Calculate class weights\n",
    "    ethnicity_classes = np.argmax(y_ethnicity_train, axis=1)\n",
    "    ethnicity_weights = compute_class_weight('balanced', classes=np.unique(ethnicity_classes), y=ethnicity_classes)\n",
    "    ethnicity_weights = dict(enumerate(ethnicity_weights))\n",
    "    \n",
    "    gender_weights = compute_class_weight('balanced', classes=[0, 1], y=y_gender_train)\n",
    "    gender_weights = dict(enumerate(gender_weights))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "        loss={\n",
    "            'age_output': 'mae',\n",
    "            'gender_output': 'binary_crossentropy',\n",
    "            'ethnicity_output': 'categorical_crossentropy'\n",
    "        },\n",
    "        loss_weights={\n",
    "            'age_output': 0.3,\n",
    "            'gender_output': 0.7,\n",
    "            'ethnicity_output': 1.0\n",
    "        },\n",
    "        metrics={\n",
    "            'age_output': ['mae', 'mse'],\n",
    "            'gender_output': ['accuracy'],\n",
    "            'ethnicity_output': ['accuracy']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=8,\n",
    "            min_lr=1e-7\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            'best_model.keras',\n",
    "            save_best_only=True,\n",
    "            monitor='val_loss'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        {\n",
    "            'age_output': y_age_train,\n",
    "            'gender_output': y_gender_train,\n",
    "            'ethnicity_output': y_ethnicity_train\n",
    "        },\n",
    "        validation_data=(\n",
    "            X_test,\n",
    "            {\n",
    "                'age_output': y_age_test,\n",
    "                'gender_output': y_gender_test,\n",
    "                'ethnicity_output': y_ethnicity_test\n",
    "            }\n",
    "        ),\n",
    "        epochs=30,\n",
    "        batch_size=64,\n",
    "        callbacks=callbacks,\n",
    "        class_weight={\n",
    "            'gender_output': gender_weights,\n",
    "            'ethnicity_output': ethnicity_weights\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(\"multi_task_model.keras\")\n",
    "    print(\"Model training complete and saved as multi_task_model.keras\")\n",
    "    \n",
    "    return model, max_age\n",
    "\n",
    "# Load trained model and make predictions\n",
    "def load_and_predict(model_path, max_age):\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    def predict(image):\n",
    "        \"\"\"Input: normalized image array (48x48x1)\"\"\"\n",
    "        predictions = model.predict(np.expand_dims(image, axis=0))\n",
    "        age = predictions[0][0][0] * max_age\n",
    "        gender = \"Male\" if predictions[1][0][0] > 0.5 else \"Female\"\n",
    "        ethnicity = np.argmax(predictions[2][0])\n",
    "        return age, gender, ethnicity\n",
    "    \n",
    "    return predict\n",
    "\n",
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    trained_model, max_age = train_model()\n",
    "    \n",
    "    # Example usage after training\n",
    "    predictor = load_and_predict(\"multi_task_model.keras\", max_age)\n",
    "    \n",
    "    # Load a sample image (replace with your image)\n",
    "    sample_image = X_test[0]  # From test set\n",
    "    age, gender, ethnicity = predictor(sample_image)\n",
    "    print(f\"Predicted Age: {age:.1f} years\")\n",
    "    print(f\"Predicted Gender: {gender}\")\n",
    "    print(f\"Predicted Ethnicity: {ethnicity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3be9287c-1f1b-4716-b92e-4d1c38ec8cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SHASHWATH M\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 22 variables whereas the saved optimizer has 42 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "model = load_model(\"multi_output_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31a341d4-42d6-4ebb-a108-fd9d656819fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Predicted Age: 35\n",
      "Predicted Gender: Female\n",
      "Predicted Ethnicity Class: 0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode=\"grayscale\", target_size=(48, 48))\n",
    "    img = img_to_array(img) / 255.0  # Normalize\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "# Path to your test image\n",
    "image_path = \"s.jpg\"\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "# Make prediction\n",
    "age_pred, gender_pred, ethnicity_pred = model.predict(input_image)\n",
    "\n",
    "# Process results\n",
    "age = round(age_pred[0][0])\n",
    "gender = \"Female\" if gender_pred[0][0] > 0.5 else \"Male\"\n",
    "ethnicity = np.argmax(ethnicity_pred[0])\n",
    "\n",
    "print(f\"Predicted Age: {age}\")\n",
    "print(f\"Predicted Gender: {gender}\")\n",
    "print(f\"Predicted Ethnicity Class: {ethnicity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b01a0b27-bf8c-480e-a771-bbbdfb801c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "\n",
    "# Modified ModelCheckpoint callback\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=8,\n",
    "        min_lr=1e-7\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'saved_models/multi_output_cnn.keras',  # Path updated\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "]\n",
    "\n",
    "# Modified model saving\n",
    "model.save(\"saved_models/multi_output_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f41c98-341d-4e8e-b9e9-370b29b14338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the top of your file\n",
    "import os\n",
    "\n",
    "# Create model directory if not exists\n",
    "MODEL_DIR = \"saved_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# In your training function\n",
    "callbacks = [\n",
    "    # ... other callbacks ...\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(MODEL_DIR, \"best_model.keras\"),\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    )\n",
    "]\n",
    "\n",
    "# After training\n",
    "final_model_path = os.path.join(MODEL_DIR, \"multi_output_cnn.keras\")\n",
    "model.save(final_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f205e6f-dd13-4ec6-a492-69c3890ef8ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_age' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# ... rest of the function ...\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Usage example\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m predictor \u001b[38;5;241m=\u001b[39m load_and_predict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/multi_output_cnn.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_age)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_age' is not defined"
     ]
    }
   ],
   "source": [
    "def load_and_predict(model_path, max_age):\n",
    "    model = load_model(model_path)\n",
    "    # ... rest of the function ...\n",
    "    \n",
    "# Usage example\n",
    "predictor = load_and_predict(\"saved_models/multi_output_cnn.keras\", max_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9abf2ba7-dd36-461c-a643-fb264978c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Add this import at the top\n",
    "\n",
    "# ... [keep all your existing code until the end] ...\n",
    "\n",
    "# Create a directory for saving models if it doesn't exist\n",
    "model_dir = \"saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model in the directory\n",
    "model.save(os.path.join(model_dir, \"multi_output_cnn.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efad6c3-81d7-4cb1-970a-9e014492bed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m592/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - age_output_loss: 32.3924 - age_output_mae: 2.5274 - ethnicity_output_accuracy: 0.4034 - ethnicity_output_loss: 1.6965 - gender_output_accuracy: 0.5806 - gender_output_loss: 0.7076 - loss: 34.7965\n",
      "Epoch 1: saving model to saved_models\\checkpoints\\model-001.keras\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 78ms/step - age_output_loss: 32.3068 - age_output_mae: 2.5233 - ethnicity_output_accuracy: 0.4035 - ethnicity_output_loss: 1.6959 - gender_output_accuracy: 0.5808 - gender_output_loss: 0.7074 - loss: 34.7101 - val_age_output_loss: 0.3886 - val_age_output_mae: 0.5738 - val_ethnicity_output_accuracy: 0.5275 - val_ethnicity_output_loss: 1.1493 - val_gender_output_accuracy: 0.7941 - val_gender_output_loss: 0.5077 - val_loss: 2.0480\n",
      "Epoch 2/20\n",
      "\u001b[1m592/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - age_output_loss: 0.7653 - age_output_mae: 0.7055 - ethnicity_output_accuracy: 0.5248 - ethnicity_output_loss: 1.2125 - gender_output_accuracy: 0.7369 - gender_output_loss: 0.5232 - loss: 2.5009\n",
      "Epoch 2: saving model to saved_models\\checkpoints\\model-002.keras\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 75ms/step - age_output_loss: 0.7649 - age_output_mae: 0.7053 - ethnicity_output_accuracy: 0.5249 - ethnicity_output_loss: 1.2122 - gender_output_accuracy: 0.7370 - gender_output_loss: 0.5231 - loss: 2.5002 - val_age_output_loss: 0.2128 - val_age_output_mae: 0.3464 - val_ethnicity_output_accuracy: 0.6161 - val_ethnicity_output_loss: 0.9337 - val_gender_output_accuracy: 0.8456 - val_gender_output_loss: 0.3715 - val_loss: 1.5221\n",
      "Epoch 3/20\n"
     ]
    }
   ],
   "source": [
    "# Add this with your other imports\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Create checkpoint callback\n",
    "checkpoint_path = os.path.join(model_dir, \"checkpoints\", \"model-{epoch:03d}.keras\")\n",
    "os.makedirs(os.path.join(model_dir, \"checkpoints\"), exist_ok=True)\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=False,\n",
    "    save_best_only=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Modify your fit call to include the callback\n",
    "model.fit(\n",
    "    X_train,\n",
    "    {\"age_output\": y_age_train, \"gender_output\": y_gender_train, \"ethnicity_output\": y_ethnicity_train},\n",
    "    validation_data=(X_test, {\"age_output\": y_age_test, \"gender_output\": y_gender_test, \"ethnicity_output\": y_ethnicity_test}),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint]  # Add this\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57840928-f14a-4d22-9e40-79237a3aaebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
